
---

## ğŸ“ Project Structure

```
search-engine/
â”œâ”€â”€ crawler/           # Crawling logic (requests, URL parsing, queue)
â”‚   â””â”€â”€ crawler.py
â”œâ”€â”€ index/             # Indexing logic (tokenization, TF-IDF)
â”‚   â””â”€â”€ indexer.py
â”œâ”€â”€ query/             # Query interface (TF-IDF scoring)
â”‚   â””â”€â”€ search.py
â”œâ”€â”€ data/              # Store crawled raw HTML or parsed text
â”‚   â””â”€â”€ pages/
â”œâ”€â”€ utils/             # Common utilities (normalization, stopwords)
â”‚   â””â”€â”€ helpers.py
â”œâ”€â”€ main.py            # Entry point
â””â”€â”€ requirements.txt   # dependencies
```

---

## âœ… Day 1: Build the Web Crawler

**Goal**: Crawl starting from your seed URLs, store HTML and extract links.

### Tasks

1. **crawler.py** (in `crawler/`)

   * Define a `Crawler` class:

     * `seed_urls: List[str]`
     * `visited: Set[str]`
     * `max_pages: int`
   * Use `requests` to fetch HTML.
   * Use `BeautifulSoup` to:

     * Parse page text (store to `data/pages`)
     * Extract all `<a href>` links.
   * Filter:

     * Ignore external domains (optional)
     * Skip duplicates, anchors, and JS links.

2. **Saving data**

   * For each visited page:

     * Save HTML to `data/pages/<hash>.html` (or plain text)
     * Optionally save metadata (e.g., original URL).

3. **Queue system**

   * Implement a basic BFS-style URL queue using a list or `deque`.

---

## âœ… Day 2: Indexing Module (TF-IDF)

**Goal**: Convert crawled pages into searchable documents.

### Tasks

1. **indexer.py** (in `index/`)

   * Load raw text.
   * Preprocess: lowercasing, remove punctuation, tokenize.
   * Remove stopwords (use `nltk.corpus.stopwords`).
   * Compute:

     * TF (term frequency)
     * DF (document frequency)
     * TF-IDF: `TF * log(N/DF)`

2. **Build Inverted Index**

   * For each token:

     * Save list of documents where it appears.
     * Include TF-IDF scores.

---

## âœ… Day 3: Query & Ranking Engine

**Goal**: Accept user query â†’ tokenize â†’ compute cosine similarity â†’ return top N results.

### Tasks

1. **search.py** (in `query/`)

   * Accept input query
   * Tokenize, clean, and compute query vector (TF-IDF)
   * Compare with indexed documents (cosine similarity)
   * Sort and return top matches with titles/snippets

---

## ğŸ”„ main.py

Tie it all together:

* `main.py crawl` â†’ start crawling
* `main.py index` â†’ build index from data/
* `main.py query "how to build a class in Python"` â†’ get ranked results

---

## âœ… Bonus: Later Extensions

* Multithreaded crawling
* Respect 'robots.txt'
* URL frontier prioritization (e.g., favor tutorial pages)
* Frontend (React or simple Flask UI)
* Query expansion / stemming
* Caching

Would you like me to begin by writing `crawler.py`?
